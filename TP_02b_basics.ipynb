{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pecZodY20n3"
      },
      "source": [
        "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZvxt-T20n4"
      },
      "source": [
        "# Module 2b: Playing with pytorch: linear regression\n",
        "\n",
        "[Video timestamp](https://youtu.be/Z6H3zakmn6E?t=960)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOgNQwiv8We1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smvq6dbY8We4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d07742b4-8e84-40e0-e8da-0a247aa3c5ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsESRui28Wgy"
      },
      "source": [
        "## Warm-up: Linear regression with numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvAi4z478Wg0"
      },
      "source": [
        "Our model is:\n",
        "$$\n",
        "y_t = 2x^1_t-3x^2_t+1, \\quad t\\in\\{1,\\dots,30\\}\n",
        "$$\n",
        "\n",
        "Our task is given the 'observations' $(x_t,y_t)_{t\\in\\{1,\\dots,30\\}}$ to recover the weights $w^1=2, w^2=-3$ and the bias $b = 1$.\n",
        "\n",
        "In order to do so, we will solve the following optimization problem:\n",
        "$$\n",
        "\\underset{w^1,w^2,b}{\\operatorname{argmin}} \\sum_{t=1}^{30} \\left(w^1x^1_t+w^2x^2_t+b-y_t\\right)^2\n",
        "$$\n",
        "\n",
        "[Video timestamp](https://youtu.be/Z6H3zakmn6E?t=1080)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hSr09Qa8Wg1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.random import random\n",
        "# generate random input data\n",
        "x = random((30,2)) #générer des données d'entrée aléatoires :\n",
        "\n",
        "# generate labels corresponding to input data x/générer des labels correspondant aux données d'entrée x, des vraies valleurs de y:\n",
        "y = np.dot(x, [2., -3.]) + 1.\n",
        "w_source = np.array([2., -3.])\n",
        "b_source  = np.array([1.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vclk_vje8Wg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd20585-22cf-4490-e928-c8cde24d47bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91088571, 0.3960003 ],\n",
              "       [0.08553416, 0.63109372],\n",
              "       [0.44810763, 0.70713165],\n",
              "       [0.75915325, 0.92049609],\n",
              "       [0.06849194, 0.14777436]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5F3fcVN8Wg6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_figs(fig_num, elev, azim, x, y, weights, bias):\n",
        "    fig = plt.figure(fig_num, figsize=(4, 3))\n",
        "    plt.clf()\n",
        "    ax = Axes3D(fig, elev=elev, azim=azim)\n",
        "    ax.scatter(x[:, 0], x[:, 1], y)\n",
        "    ax.plot_surface(np.array([[0, 0], [1, 1]]),\n",
        "                    np.array([[0, 1], [0, 1]]),\n",
        "                    (np.dot(np.array([[0, 0, 1, 1],\n",
        "                                          [0, 1, 0, 1]]).T, weights) + bias).reshape((2, 2)),\n",
        "                    alpha=.5)\n",
        "    ax.set_xlabel('x_1')\n",
        "    ax.set_ylabel('x_2')\n",
        "    ax.set_zlabel('y')\n",
        "    \n",
        "def plot_views(x, y, w, b):\n",
        "    #Generate the different figures from different views/ génère différentes figures issues des différents vues :\n",
        "    elev = 43.5\n",
        "    azim = -110\n",
        "    plot_figs(1, elev, azim, x, y, w, b[0])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l7XSyJM8Wg8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "087adb29-2f45-4558-ff8b-cf52c992d37b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x300 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_views(x, y, w_source, b_source) \n",
        "#Notre tâche est de représenter l'équation des points(observation) du plan et miniser la MSE. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZAMgQn8Wg9"
      },
      "source": [
        "In vector form, we define:\n",
        "$$\n",
        "\\hat{y}_t = {\\bf w}^T{\\bf x}_t+b\n",
        "$$\n",
        "and we want to minimize the loss given by:\n",
        "$$\n",
        "loss = \\sum_t\\underbrace{\\left(\\hat{y}_t-y_t \\right)^2}_{loss_t}.\n",
        "$$\n",
        "\n",
        "To minimize the loss we first compute the gradient of each $loss_t$:\n",
        "\\begin{eqnarray*}\n",
        "\\frac{\\partial{loss_t}}{\\partial w^1} &=& 2x^1_t\\left({\\bf w}^T{\\bf x}_t+b-y_t \\right)\\\\\n",
        "\\frac{\\partial{loss_t}}{\\partial w^2} &=& 2x^2_t\\left({\\bf w}^T{\\bf x}_t+b-y_t \\right)\\\\\n",
        "\\frac{\\partial{loss_t}}{\\partial b} &=& 2\\left({\\bf w}^T{\\bf x}_t+b-y_t \\right)\n",
        "\\end{eqnarray*}\n",
        "\n",
        "Note that the actual gradient of the loss is given by:\n",
        "$$\n",
        "\\frac{\\partial{loss}}{\\partial w^1} =\\sum_t \\frac{\\partial{loss_t}}{\\partial w^1},\\quad\n",
        "\\frac{\\partial{loss}}{\\partial w^2} =\\sum_t \\frac{\\partial{loss_t}}{\\partial w^2},\\quad\n",
        "\\frac{\\partial{loss}}{\\partial b} =\\sum_t \\frac{\\partial{loss_t}}{\\partial b}\n",
        "$$\n",
        "\n",
        "For one epoch, **(Batch) Gradient Descent** updates the weights and bias as follows:\n",
        "\\begin{eqnarray*}\n",
        "w^1_{new}&=&w^1_{old}-\\alpha\\frac{\\partial{loss}}{\\partial w^1} \\\\\n",
        "w^2_{new}&=&w^2_{old}-\\alpha\\frac{\\partial{loss}}{\\partial w^2} \\\\\n",
        "b_{new}&=&b_{old}-\\alpha\\frac{\\partial{loss}}{\\partial b},\n",
        "\\end{eqnarray*}\n",
        "\n",
        "and then we run several epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2eUfQws8WhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd9b714-0ef0-493a-b575-b0d0802a1480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial values of the parameters: [0.50720218 0.25600937] [0.42059718]\n"
          ]
        }
      ],
      "source": [
        "# randomly initialize learnable weights and bias/on inialise de manière aléatoire les poids et biais apprenables: \n",
        "w_init = random(2)\n",
        "b_init = random(1)\n",
        "\n",
        "w = w_init\n",
        "b = b_init\n",
        "print(\"initial values of the parameters:\", w, b )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1kUBCujB_jk",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cd289b-dd77-415a-f106-77c1a8ffcf58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 29.433702438427858\n",
            "progress: epoch: 0 loss 29.43370243842785\n",
            "progress: epoch: 1 loss 26.33138530704353\n",
            "progress: epoch: 2 loss 24.01898035972118\n",
            "progress: epoch: 3 loss 21.913359101698493\n",
            "progress: epoch: 4 loss 19.99572378103657\n",
            "progress: epoch: 5 loss 18.249078726534336\n",
            "progress: epoch: 6 loss 16.657977915249866\n",
            "progress: epoch: 7 loss 15.208382870425982\n",
            "progress: epoch: 8 loss 13.887533651351326\n",
            "progress: epoch: 9 loss 12.68383171264598\n",
            "estimation of the parameters: [ 1.14357401 -0.90209249] [0.46694245]\n"
          ]
        }
      ],
      "source": [
        "#Ici on définit 3 prinpales fonctions: \n",
        "# our model forward pass/ la première fonction (y^t=wTxt+b)\n",
        "def forward(x):\n",
        "    return x.dot(w)+b\n",
        "\n",
        "# Loss function/ fonction de perte \n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y)**2 \n",
        "\n",
        "print(\"initial loss:\", np.sum([loss(x_val,y_val) for x_val, y_val in zip(x, y)]) )\n",
        "\n",
        "# compute gradient (compute les dérivés du gradient)\n",
        "def gradient(x, y):  # d_loss/d_w, d_loss/d_c\n",
        "    return 2*(x.dot(w)+b - y)*x, 2 * (x.dot(w)+b - y)\n",
        " #première dérivé de la perte qui respecte w, et la second dérivé de la perte qui resptect le bias\n",
        "learning_rate = 1e-2\n",
        "# Training loop\n",
        "for epoch in range(10): #si nous voulons un meilleur résultat il faut plus d'époch passer de 10 à 50 expl\n",
        "    grad_w = np.array([0,0])\n",
        "    grad_b = np.array(0)\n",
        "    l = 0\n",
        "    for x_val, y_val in zip(x, y):\n",
        "        grad_w = np.add(grad_w,gradient(x_val, y_val)[0])\n",
        "        grad_b = np.add(grad_b,gradient(x_val, y_val)[1])\n",
        "        l += loss(x_val, y_val)\n",
        "    w = w - learning_rate * grad_w\n",
        "    b = b - learning_rate * grad_b\n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",l[0])\n",
        "\n",
        "# After training\n",
        "print(\"estimation of the parameters:\", w, b) #la perte décroit , si on veut de meilleur résultat il faut plus d'epoch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO_DKaes8WhB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50348689-e07e-4aee-d9c8-1224b70df7d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x300 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_views(x, y, w, b) #approximation d'équation pour le plan en 3 dim "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQz2GoXh8WhC"
      },
      "source": [
        "## Linear regression with tensors\n",
        "\n",
        "[Video timestamp](https://youtu.be/Z6H3zakmn6E?t=1650)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KKfUKVn8WhD"
      },
      "outputs": [],
      "source": [
        "dtype = torch.FloatTensor\n",
        "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLQeUOFi8WhF"
      },
      "outputs": [],
      "source": [
        "#la première chose à faire est de transférer tout de numpy vers tensor . \n",
        "x_t = torch.from_numpy(x).type(dtype)\n",
        "y_t = torch.from_numpy(y).type(dtype).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MExDafv8WhG"
      },
      "source": [
        "This is an implementation of **(Batch) Gradient Descent** with tensors.\n",
        "\n",
        "Note that in the main loop, the functions loss_t and gradient_t are always called with the same inputs: they can easily be incorporated into the loop (we'll do that below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tWn-6PC8WhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54958f56-1ae1-4338-9666-72f99e39cfb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial values of the parameters: tensor([[0.5072],\n",
            "        [0.2560]]) tensor([[0.4206]])\n"
          ]
        }
      ],
      "source": [
        "w_init_t = torch.from_numpy(w_init).type(dtype)\n",
        "b_init_t = torch.from_numpy(b_init).type(dtype)\n",
        "#Les fonctions loss et gradient sont appelées avec les mêmes estimations initiales (w et b)\n",
        "w_t = w_init_t.clone()\n",
        "w_t.unsqueeze_(1)\n",
        "b_t = b_init_t.clone()\n",
        "b_t.unsqueeze_(1)\n",
        "print(\"initial values of the parameters:\", w_t, b_t )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ovfMuTVB_jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f136a3b9-5438-430d-dd27-c27e3f721c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss tensor(29.4337)\n",
            "progress: epoch: 1 loss tensor(26.3314)\n",
            "progress: epoch: 2 loss tensor(24.0190)\n",
            "progress: epoch: 3 loss tensor(21.9134)\n",
            "progress: epoch: 4 loss tensor(19.9957)\n",
            "progress: epoch: 5 loss tensor(18.2491)\n",
            "progress: epoch: 6 loss tensor(16.6580)\n",
            "progress: epoch: 7 loss tensor(15.2084)\n",
            "progress: epoch: 8 loss tensor(13.8875)\n",
            "progress: epoch: 9 loss tensor(12.6838)\n",
            "estimation of the parameters: tensor([[ 1.1436],\n",
            "        [-0.9021]]) tensor([[0.4669]])\n"
          ]
        }
      ],
      "source": [
        "# our model forward pass\n",
        "def forward_t(x):\n",
        "    return x.mm(w_t)+b_t #ici seule modif (avec la fonction faite avec numpy) : on utilise non pas dot mais la matrice multiplication : mm\n",
        "\n",
        "# Loss function\n",
        "def loss_t(x, y):\n",
        "    y_pred = forward_t(x) #compile les prédictions fait sur la boucle x\n",
        "    return (y_pred - y).pow(2).sum() # je les compare avec les vraies valeurs : y et somme au carré de pytorch \n",
        "\n",
        "# compute gradient\n",
        "def gradient_t(x, y):  # d_loss/d_w, d_loss/d_c\n",
        "    return 2*torch.mm(torch.t(x),x.mm(w_t)+b_t - y), 2 * (x.mm(w_t)+b_t - y).sum()\n",
        "#la dérivé de la perte qui respecte le poids et la dérivé de la perte qui respecte le bias\n",
        "learning_rate = 1e-2\n",
        "for epoch in range(10):\n",
        "    l_t = loss_t(x_t,y_t)\n",
        "    grad_w, grad_b = gradient_t(x_t,y_t)\n",
        "    w_t = w_t-learning_rate*grad_w\n",
        "    b_t = b_t-learning_rate*grad_b\n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",l_t)\n",
        "\n",
        "# After training\n",
        "print(\"estimation of the parameters:\", w_t, b_t )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTrqi-ux8WhJ"
      },
      "source": [
        "## Linear regression with Autograd\n",
        "\n",
        "[Video timestamp](https://youtu.be/Z6H3zakmn6E?t=1890)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTH6VOMz8WhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3fd971-ee19-486d-cd87-34836814334b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial values of the parameters: tensor([[0.5072],\n",
            "        [0.2560]]) tensor([[0.4206]])\n"
          ]
        }
      ],
      "source": [
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass./on veut compiler le gradient par rapport à ses tensors durant le backward pass\n",
        "#cette partie de code initialise les poids et le biais de la régression linéaire, les marque comme nécessitant le calcul des gradients,\n",
        "# et affiche les valeurs initiales des paramètres. Cela prépare le modèle pour l'apprentissage et la mise à jour des poids et du biais \n",
        "#à l'aide de la rétropropagation. \n",
        "w_v = w_init_t.clone().unsqueeze(1) #on crée une copie du tenseur w_init\n",
        "w_v.requires_grad_(True) #nous souhaitons calculer les gradients par rapport à ce tenseur lors de la rétropropagation (backward pass)\n",
        "b_v = b_init_t.clone().unsqueeze(1) #on crée une copie du tenseur b_init\n",
        "b_v.requires_grad_(True)\n",
        "print(\"initial values of the parameters:\", w_v.data, b_v.data )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHgm3N8Y8WhK"
      },
      "source": [
        "An implementation of **(Batch) Gradient Descent** without computing explicitly the gradient and using autograd instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4sdOF0e8WhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1cda62-b2cd-46fd-ed23-e1e304171c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 29.433704376220703\n",
            "progress: epoch: 1 loss 26.331384658813477\n",
            "progress: epoch: 2 loss 24.018978118896484\n",
            "progress: epoch: 3 loss 21.913360595703125\n",
            "progress: epoch: 4 loss 19.995723724365234\n",
            "progress: epoch: 5 loss 18.24907875061035\n",
            "progress: epoch: 6 loss 16.657978057861328\n",
            "progress: epoch: 7 loss 15.208383560180664\n",
            "progress: epoch: 8 loss 13.887533187866211\n",
            "progress: epoch: 9 loss 12.683832168579102\n",
            "estimation of the parameters: tensor([[ 1.1436],\n",
            "        [-0.9021]]) tensor([[0.4669]])\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    y_pred = x_t.mm(w_v)+b_v\n",
        "    loss = (y_pred - y_t).pow(2).sum()\n",
        "    \n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
        "    # After this call w.grad and b.grad will be tensors holding the gradient\n",
        "    # of the loss with respect to w and b respectively.\n",
        "\n",
        "    #on utilise l'autograd pour calculer le bacward pass.On calculera le gradient du loss par rapport \n",
        "    #à toutes les variables\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update weights using gradient descent. For this step we just want to mutate\n",
        "    # the values of w_v and b_v in-place; we don't want to build up a computational\n",
        "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
        "    # to prevent PyTorch from building a computational graph for the updates\n",
        "    with torch.no_grad(): #on empèche torc de construire un graph computationnel pour les mises à jours de poids, on économise en mémoire \n",
        "        w_v -= learning_rate * w_v.grad #soustrait le produit du taux d'apprentissage (learning_rate) et du gradient de w_v à la valeur actuelle de w_v. \n",
        "        b_v -= learning_rate * b_v.grad #on fait pareil ici avec b_v\n",
        "    \n",
        "    # Manually zero the gradients after updating weights\n",
        "    # otherwise gradients will be acumulated after each .backward()\n",
        "    #ne pas oublier de remettre les gradients à zéro pour éviter que les gradients calculés lors de la mise à jour précédente \n",
        "    #ne s'accumulent avec les gradients calculés lors des prochaines itérations de la descente de gradient.\n",
        "    w_v.grad.zero_()\n",
        "    b_v.grad.zero_()\n",
        "    \n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.data.item())\n",
        "\n",
        "# After training\n",
        "print(\"estimation of the parameters:\", w_v.data, b_v.data.t() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W720TY4R8WhN"
      },
      "source": [
        "## Linear regression with neural network\n",
        "\n",
        "[Video timestamp](https://youtu.be/Z6H3zakmn6E?t=2075)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VgMVkn48WhN"
      },
      "source": [
        "An implementation of **(Batch) Gradient Descent** using the nn package. Here we have a super simple model with only one layer and no activation function!\n",
        "On crée donc un modèle linéaire simple , avec une couche linaire à deux entrées (features) et une sortie. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plEHj42d8WhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d032dc89-f3c3-4365-e97a-0ffc6cf743be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 29.433704376220703\n",
            "progress: epoch: 1 loss 26.331384658813477\n",
            "progress: epoch: 2 loss 24.018978118896484\n",
            "progress: epoch: 3 loss 21.913360595703125\n",
            "progress: epoch: 4 loss 19.995723724365234\n",
            "progress: epoch: 5 loss 18.24907875061035\n",
            "progress: epoch: 6 loss 16.657978057861328\n",
            "progress: epoch: 7 loss 15.208383560180664\n",
            "progress: epoch: 8 loss 13.887533187866211\n",
            "progress: epoch: 9 loss 12.683832168579102\n",
            "estimation of the parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 1.1436, -0.9021]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4669], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Variables for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1),\n",
        ")\n",
        "\n",
        "for m in model.children(): #on fait une boucle pour parcourir chaque module enfant du modèle (ici une seule couche linéaire)\n",
        "    m.weight.data = w_init_t.clone().unsqueeze(0) #on initialise avec les valeurs clonées \n",
        "    m.bias.data = b_init_t.clone()\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum') #on crée une instance (torch.nn.MSELoss) qui représente la fonction de perte \n",
        "#pour une régression linéaire basée sur la MSE.\n",
        "\n",
        "# switch to train mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Variable of input data to the Module and it produces\n",
        "    # a Variable of output data.\n",
        "    y_pred = model(x_t) #on entraine le modèle sur 10 epoch et on effectue les prédictions sur les données d'entrée (x_t)\n",
        "  \n",
        "    # Note this operation is equivalent to: pred = model.forward(x_v)\n",
        "\n",
        "    # Compute and print loss. We pass Variables containing the predicted and true\n",
        "    # values of y, and the loss function returns a Variable containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y_t) #on calcule la perte (sachant que l'objectif de l'entrainement est de minimiser cette perte)\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad() #réinitialiser les gradients à zéro à chaque itération pour éviter l'accumulation de gradients indésirables \n",
        "    #provenant d'itérations précédentes.\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Variables with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its data and gradients like we did before.\n",
        "    #on met à jour les param en utilisant la méthode de descente de gradients \n",
        "    with torch.no_grad(): #opération non prise en compte dans le backward et n'affecte pas le calcul de nouveaux gradients\n",
        "        for param in model.parameters():\n",
        "            param.data -= learning_rate * param.grad\n",
        "        \n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.data.item())\n",
        "\n",
        "# After training\n",
        "print(\"estimation of the parameters:\")\n",
        "for param in model.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gus-YF9p8WhP"
      },
      "source": [
        "Last step, we use directly the optim package to update the weights and bias.\n",
        "Le package optim de PyTorch fournit des implémentations d'algorithmes d'optimisation couramment utilisés pour mettre à jour les poids et les biais (paramètres) d'un modèle lors de l'entraînement\n",
        "\n",
        "[Video timestamp](https://youtu.be/Z6H3zakmn6E?t=2390)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VbF5gYV8WhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422acbc2-2a5d-49e6-f65a-ec8c0da4d4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 29.433704376220703\n",
            "progress: epoch: 1 loss 26.331384658813477\n",
            "progress: epoch: 2 loss 24.018978118896484\n",
            "progress: epoch: 3 loss 21.913358688354492\n",
            "progress: epoch: 4 loss 19.995725631713867\n",
            "progress: epoch: 5 loss 18.24907875061035\n",
            "progress: epoch: 6 loss 16.657978057861328\n",
            "progress: epoch: 7 loss 15.208383560180664\n",
            "progress: epoch: 8 loss 13.887533187866211\n",
            "progress: epoch: 9 loss 12.683832168579102\n",
            "estimation of the parameters:\n",
            "Parameter containing:\n",
            "tensor([[ 1.1436, -0.9021]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.4669], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1),\n",
        ")\n",
        "\n",
        "for m in model.children():\n",
        "    m.weight.data = w_init_t.clone().unsqueeze(0)\n",
        "    m.bias.data = b_init_t.clone()\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    y_pred = model(x_t)\n",
        "    loss = loss_fn(y_pred, y_t)\n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.item())\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "# After training\n",
        "print(\"estimation of the parameters:\")\n",
        "for param in model.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFxayCILB_j2"
      },
      "source": [
        "## Remark\n",
        "\n",
        "This problem can be solved in 3 lines of code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q9sz3M8B_j2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf81bdb-b5fa-49de-f1f4-d04f687a281e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.0000],\n",
              "        [-3.0000],\n",
              "        [ 1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "xb_t = torch.cat((x_t,torch.ones(30).unsqueeze(1)),1)\n",
        "# for old version of pytorch\n",
        "#sol, _ =torch.lstsq(y_t,xb_t)\n",
        "#sol[:3]\n",
        "# for pytorch 1.9 and newer\n",
        "sol = torch.linalg.lstsq(xb_t,y_t)\n",
        "sol.solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY_I9v3o8WhQ"
      },
      "source": [
        "## Exercise: Play with the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It_tRuXs8WhQ"
      },
      "source": [
        "Change the number of samples from 30 to 300. What happens? How to correct it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRk1zgSv8WhR"
      },
      "outputs": [],
      "source": [
        "x = random((300,2))\n",
        "y = np.dot(x, [2., -3.]) + 1.\n",
        "x_t = torch.from_numpy(x).type(dtype)\n",
        "y_t = torch.from_numpy(y).type(dtype).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-xOCI6z8WhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c78abb-fb82-48f9-e869-b0d3fb5f7e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 358.25164794921875\n",
            "progress: epoch: 1 loss 4185.1220703125\n",
            "progress: epoch: 2 loss 281676.59375\n",
            "progress: epoch: 3 loss 19350164.0\n",
            "progress: epoch: 4 loss 1329407360.0\n",
            "progress: epoch: 5 loss 91333804032.0\n",
            "progress: epoch: 6 loss 6274877489152.0\n",
            "progress: epoch: 7 loss 431100765667328.0\n",
            "progress: epoch: 8 loss 2.961778094060339e+16\n",
            "progress: epoch: 9 loss 2.034820402353537e+18\n",
            "estimation of the parameters:\n",
            "Parameter containing:\n",
            "tensor([[2.2745e+08, 2.4235e+08]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([4.3650e+08], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1),\n",
        ")\n",
        "\n",
        "for m in model.children():\n",
        "    m.weight.data = w_init_t.clone().unsqueeze(0)\n",
        "    m.bias.data = b_init_t.clone()\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
        "\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    y_pred = model(x_t)\n",
        "    loss = loss_fn(y_pred, y_t)\n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.item())\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "# After training\n",
        "print(\"estimation of the parameters:\")\n",
        "for param in model.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quand on modifie le nombre d'échantillon de 30 à 300, les dimensions des données d'entrée (x) et des données cibles (y) doivent être ajustées en conséquence. Dans le code donné, x a une forme de (300, 2) et y a une forme de (300,), ce qui est incompatible avec les dimensions attendues par le modèle et la fonction de perte.\n",
        "\n",
        "Pour corriger cela, on doit générer x et y avec les dimensions correctes :\n",
        "x = np.random.random((300, 2))\n",
        "y = np.dot(x, [2., -3.]) + 1."
      ],
      "metadata": {
        "id": "NsgomTzGzWhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random((300, 2))\n",
        "y = np.dot(x, [2., -3.]) + 1\n",
        "x_t = torch.from_numpy(x).type(dtype)\n",
        "y_t = torch.from_numpy(y).type(dtype).unsqueeze(1)"
      ],
      "metadata": {
        "id": "ii2gTcyQzPv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1),\n",
        ")\n",
        "\n",
        "for m in model.children():\n",
        "    m.weight.data = w_init_t.clone().unsqueeze(0)\n",
        "    m.bias.data = b_init_t.clone()\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
        "\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    y_pred = model(x_t)\n",
        "    loss = loss_fn(y_pred, y_t)\n",
        "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.item())\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "# After training\n",
        "print(\"estimation of the parameters:\")\n",
        "for param in model.parameters():\n",
        "    print(param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLutY21LzhQD",
        "outputId": "6d6a3375-31f8-4502-b0c4-733c292598ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: epoch: 0 loss 328.1256103515625\n",
            "progress: epoch: 1 loss 2330.90478515625\n",
            "progress: epoch: 2 loss 154817.109375\n",
            "progress: epoch: 3 loss 10684682.0\n",
            "progress: epoch: 4 loss 737528320.0\n",
            "progress: epoch: 5 loss 50909188096.0\n",
            "progress: epoch: 6 loss 3514096943104.0\n",
            "progress: epoch: 7 loss 242566700204032.0\n",
            "progress: epoch: 8 loss 1.674359714349056e+16\n",
            "progress: epoch: 9 loss 1.1557560455832535e+18\n",
            "estimation of the parameters:\n",
            "Parameter containing:\n",
            "tensor([[1.8096e+08, 1.7505e+08]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([3.2867e+08], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAiS-TfqDOMU"
      },
      "source": [
        "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}